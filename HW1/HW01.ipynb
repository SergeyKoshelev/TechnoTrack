{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ №1 - обучение модели линейной регресии методом градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать обучение модели линейной регрессии методом градиентного спуска.<br />\n",
    "\n",
    "В качестве подводящего упражнения в этом задании предлагается реализовать функции потерь и саму модель линейной регрессии в манере, схожей с построением модулей фреймворка pytorch (см. пояснения в шаблонах кода)\n",
    "\n",
    "В решении ожидается наличие следующих ключевых составляющих:<br />\n",
    "\n",
    "#### Текстовое описание в решении:\n",
    "- формулировка задачи, формулировка признакового описания объектов, формулировка функции ошибки, формулировка меры качества модели;\n",
    "- исследование исходных данных на предмет скоррелированности признаков; фильтрация признаков; порождение признаков (при необходимости);\n",
    "- оценка параметров модели линейной регрессии (обучение модели) методом градиентного спуска;\n",
    "- оценка качества модели на валидационной выборке.\n",
    "\n",
    "#### Код решения:\n",
    "(используйте предлагаемые шаблоны)\n",
    "- формулировка модели линейной регрессии;\n",
    "- формулировка функции ошибки;\n",
    "- формулировка метрики (метрик);\n",
    "- формулировка цикла оптимизации параметров.\n",
    "\n",
    "\n",
    "#### Визуализация в решении:\n",
    "- распределение признаков;\n",
    "- распределение целевой переменной;\n",
    "- эволюция функции ошибки и выбранных метрик качества по ходу обучения.\n",
    "\n",
    "#### Выводы (в форме текста!)\n",
    "- вывод о том, насколько модель подходит для описания данных\n",
    "- вывод о достаточности или избыточности данных для оценки параметров модели\n",
    "- вывод о соотношении выразительности модели и ее обобщающей способности (наблюдаются ли явления переобучения или недообучения).\n",
    "\n",
    "Примечания:<br />\n",
    "Допустимо порождение признаков (полиномиальных, экспоненциальных, логарифмических, etc.)<br />\n",
    "Реализация линейной регрессии может быть написана только с использованием библиотеки Numpy. Решения с использованием библиотек автоматического вычисления градиентов не засчитываются.<br />\n",
    "Из готовых реализаций (напр., из пакета scikit-learn) в этом задании допускается использовать только порождение полиномиальных признаков `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные находятся в следующих файлах:\n",
    "\n",
    "Признаковое описание объектов обучающей выборки - в файле X_train.npy\n",
    "\n",
    "Значения целевой переменной на обучающей выборке - в файле y_train.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Способ чтения данных из файлов *.npy :\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "data = np.load('/path/to/filename.npy')\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примечание на предмет реализации градиента функции потерь\n",
    "\n",
    "Нелишним будет вспомнить способ вычисления градиента сложной функции. Здесь функция ошибки (обозначено как $\\mathscr{L}$) представлена как сложная функция $\\mathscr{L}\\left( G\\left( \\theta \\right) \\right)$. Для простоты приведена сразу матричная запись.\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}{\\mathscr{L}} = \\nabla_{\\theta}{G}\\cdot\\left(\\nabla_{G}{\\mathscr{L}}\\right)\n",
    "$$\n",
    "\n",
    "В качестве шпаргалки можно подсмотреть правила матричного дифференцирования <a href=\"https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\">здесь</a>\n",
    "\n",
    "Например, в случае функции потерь MSE это может выглядеть следующим образом:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta}{\\mathscr{L}} = X^T\\cdot2\\left(X\\theta - Y\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "При этом логически имеет смысл реализовать компоненту градиента, относящуюся собственно к функции потерь $\\nabla_{G}{\\mathscr{L}}$ - в коде класса функции потерь, а компоненту, относящуюся к модели $\\nabla_{\\theta}{G}$ - в коде модели.\n",
    "\n",
    "Именно поэтому классы `loss` и `linear_regression` в предложенном шаблоне реализованы наследующими `Differentiable` - для общности восприятия этих модулей как дифференцируемых по своим аргументам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = np.load('./X_train.npy')\n",
    "ytr = np.load('./y_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Differentiable:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def backward(self, **kwargs):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loss(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(loss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Этот метод реализует вычисление значения функции потерь\n",
    "        # Подсказка: метод должен возвращать единственный скаляр - значение функции потерь\n",
    "        loss_value = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # loss_value = ...\n",
    "                \n",
    "        return loss_value\n",
    "    \n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        # Этот метод реализует вычисление градиента функции потерь по аргументу y_pred\n",
    "        # Подсказка: метод должен возвращать вектор градиента функции потерь\n",
    "        #           размерностью, совпадающей с размерностью аргумента y_pred\n",
    "        \n",
    "        partial_grad = np.zeros_like(y_pred-y_true)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # partial_grad = ...\n",
    "        \n",
    "        return partial_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression(Differentiable):\n",
    "    def __init__(self):\n",
    "        super(linear_regression, self).__init__()\n",
    "        self.theta = None\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        # этот метод предназначен для вычисления значения целевой переменной\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, X):\n",
    "        # Этот метод реализует вычисление компоненты градиента функции потерь\n",
    "        \n",
    "        partial_grad = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # partial_grad = ...\n",
    "        \n",
    "        return partial_grad\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # этот метод предназначен для применения модели к данным\n",
    "        assert X.ndim == 2, \"X should be 2-dimensional: (N of objects, n of features)\"\n",
    "        \n",
    "        if (self.theta is None):\n",
    "            # Если вектор параметров еще не инициализирован, его следует инициализировать\n",
    "            # Подсказка: длина вектора параметров может быть получена из размера матрицы X\n",
    "            self.theta = 0.0\n",
    "            \n",
    "            ### YOUR CODE HERE\n",
    "            # self.theta = ...\n",
    "        \n",
    "        \n",
    "        # Здесь следует собственно применить модель к входным данным\n",
    "        # Подсказка: удобно расширить матрицу X дополнительным признаком,\n",
    "        #            чтобы применять матричные операции, очень эффективно реализованные в numpy\n",
    "        \n",
    "        y_pred = 0.0\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # y_pred = ...\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(X, y, model, loss_fn, epochs=100):\n",
    "    loss_history = []\n",
    "    pbar = tqdm(total=epochs)\n",
    "    for epoch in range(epochs):\n",
    "        # В этом цикле следует реализовать итеративную процедуру оптимизации параметров модели model,\n",
    "        #        руководствуясь функцией потерь loss_fn\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        # loss_value = ...\n",
    "        # grad = ...\n",
    "        # model.theta = ...\n",
    "        \n",
    "        loss_history.append(loss_value)\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({'loss': loss_value})\n",
    "    pbar.close()\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_fn = loss()\n",
    "lr_model = linear_regression()\n",
    "loss_history = train_loop(Xtr, ytr, lr_model, obj_fn, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
